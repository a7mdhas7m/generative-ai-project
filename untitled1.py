# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EVn3LCPnw5GWh7heIH3Hrw96d5wDVN2F
"""

# this is the required libraries
!pip install transformers datasets --quiet

# Upload the new big dataset file
from google.colab import files
uploaded = files.upload()  # upload long_stories_40k.txt here

# Import necessary modules
from datasets import Dataset
from transformers import GPT2Tokenizer

# Loading the new large text data
file_path = "long_stories_40k.txt"
with open(file_path, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

# Convert to the Hugging Face Dataset
dataset_dict = {"text": lines}
dataset = Dataset.from_dict(dataset_dict)

# Tokenization
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # Set pad token

def tokenize(example):
    return tokenizer(example["text"], truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=["text"])

# Load GPT-2 model and data collator
from transformers import GPT2LMHeadModel, DataCollatorForLanguageModeling

model = GPT2LMHeadModel.from_pretrained("gpt2")
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Training configuration
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./gpt2-long-stories-finetuned",
    num_train_epochs=2,                  # You can adjust epochs here
    per_device_train_batch_size=4,       # Adjust based on GPU memory
    save_steps=500,
    logging_steps=100,
    save_total_limit=1,
    prediction_loss_only=True,
    report_to="none"
)

# Start training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("/content/gpt2-long-stories-final")
tokenizer.save_pretrained("/content/gpt2-long-stories-final")

# Reload the fine-tuned model for generation
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model_path = "/content/gpt2-long-stories-final"
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)

# Function to generate story/quote
def generate_quote(prompt, max_length=50):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        do_sample=True,
        max_length=max_length,
        top_k=50,
        top_p=0.9,
        temperature=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# (Optional) You can customize or remove this category detector or modify it for story prompts
def detect_category(user_input):
    text = user_input.lower()
    if any(word in text for word in ["adventure", "journey", "quest", "explore"]):
        return "[ADVENTURE]"
    elif any(word in text for word in ["fantasy", "magic", "dragon", "wizard"]):
        return "[FANTASY]"
    elif any(word in text for word in ["space", "future", "robot", "alien"]):
        return "[SCI-FI]"
    elif any(word in text for word in ["horror", "ghost", "fear", "darkness"]):
        return "[HORROR]"
    else:
        return "[GENERAL]"

# User prompt input
print("Enter your story prompt (e.g. adventure in the mountains, magic castle, alien invasion, haunted house):")
user_input = input("Your prompt: ")

# Add category tag automatically
category_token = detect_category(user_input)
final_prompt = f"{category_token} {user_input}"

# Generate and print the result
result = generate_quote(final_prompt)
print("\nGenerated Story:")
print(result)

# Compress the model folder into a zip file
from shutil import make_archive
make_archive("/content/gpt2-long-stories-final", 'zip', "/content/gpt2-long-stories-final")

